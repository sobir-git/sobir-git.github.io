<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My Notes - misc</title><link href="/" rel="alternate"></link><link href="/feeds/misc.atom.xml" rel="self"></link><id>/</id><updated>2021-05-03T00:00:00+03:00</updated><entry><title>Remembering formula of normal distribution</title><link href="/remembering-formula-of-normal-distribution.html" rel="alternate"></link><published>2021-05-03T00:00:00+03:00</published><updated>2021-05-03T00:00:00+03:00</updated><author><name>Sobir Bobiev</name></author><id>tag:None,2021-05-03:/remembering-formula-of-normal-distribution.html</id><summary type="html">&lt;p&gt;When googled, I got this for the normal distribution:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="/images/normal_distribution.svg" /&gt;
&lt;/p&gt;

&lt;p&gt;You can see it in many other different forms. I will try my best to show at best how can you remember it, or at least what you should keep in mind.&lt;/p&gt;
&lt;p&gt;It breaks down into two parts:
$$
\frac{1}{\sigma …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When googled, I got this for the normal distribution:&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="/images/normal_distribution.svg" /&gt;
&lt;/p&gt;

&lt;p&gt;You can see it in many other different forms. I will try my best to show at best how can you remember it, or at least what you should keep in mind.&lt;/p&gt;
&lt;p&gt;It breaks down into two parts:
$$
\frac{1}{\sigma\sqrt{2\pi}}
$$
and
$$
e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$&lt;/p&gt;
&lt;p&gt;The first part is just a &lt;strong&gt;normalizing constant&lt;/strong&gt; that is worth memorizing.&lt;/p&gt;
&lt;p&gt;Let's break down the &lt;strong&gt;second part&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Exponent $e$: the normal distribution converges asymptotically to 0 &lt;strong&gt;exponentially&lt;/strong&gt; on both tails, or more accurately &lt;em&gt;quadratic exponentially&lt;/em&gt; (if that's is a term).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Negative sign in the exponent: without which it would diverge on both tails.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The fraction $(\frac{x-\mu}{\sigma})^2$: it this is the part that gives it parametrization of different means and standard deviations. The numerator $x - \mu$ ensures that at it has the peak at $x = \mu$ and the denominator $\sigma$ is used to stretch the x-axis linearly, so that desired standard deviation is obtained. Without the square the distribution wouldn't be symmetric.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The fraction $\frac{1}{2}$: sure, it would look cooler without this fraction, but the standard deviation would be $\sigma / \sqrt{2}$ instead of just $\sigma$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="misc"></category></entry><entry><title>Hypothesis Testing</title><link href="/hypothesis-testing.html" rel="alternate"></link><published>2021-05-01T00:00:00+03:00</published><updated>2021-05-01T00:00:00+03:00</updated><author><name>Sobir Bobiev</name></author><id>tag:None,2021-05-01:/hypothesis-testing.html</id><summary type="html">&lt;p&gt;The following are my notes when I studied &lt;a href="https://www.statlect.com/fundamentals-of-statistics/hypothesis-testing"&gt;this chapter&lt;/a&gt; on hypothesis testing.&lt;/p&gt;
&lt;p&gt;First we define what is a &lt;strong&gt;statistical model&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Statistical Models&lt;/h3&gt;
&lt;p&gt;The term didn't catch attention, as if I already knew it. Perhaps true. I know both words in isolation, and I've seen both together. But here follows …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The following are my notes when I studied &lt;a href="https://www.statlect.com/fundamentals-of-statistics/hypothesis-testing"&gt;this chapter&lt;/a&gt; on hypothesis testing.&lt;/p&gt;
&lt;p&gt;First we define what is a &lt;strong&gt;statistical model&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Statistical Models&lt;/h3&gt;
&lt;p&gt;The term didn't catch attention, as if I already knew it. Perhaps true. I know both words in isolation, and I've seen both together. But here follows the formal definition.&lt;/p&gt;
&lt;p&gt;Given an unknown distribution. Assume we only have some samples from it. Of course we cannot characterize the original distribution just from a finite number of samples. We can only infer some properties of it given particular circumstances. For example we assume that it belongs to a certain class of probability distributions. If we assume it is a normal distribution, then we go further and estimate its mean and variance, or make a probabilistic statements about it. &lt;/p&gt;
&lt;p&gt;So, the class which we assumed the unknown distributions belongs to is called our &lt;em&gt;statistical model&lt;/em&gt;. We could have modeled it correctly, in which case we say our model is &lt;strong&gt;correctly specified&lt;/strong&gt;. Otherwise, it is &lt;strong&gt;mis-specified&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Statistical Inference&lt;/h3&gt;
&lt;p&gt;A statistical inference is a statement about the population from a given sample. They are based on a given sample and the statistical model. A statistical inference has the form of model restriction. Let the original model be $\Phi$. Then the statistical inference can talk about a subset of model, $\Phi_R$, in one of the following forms: (a) the unknown distribution belongs to $\Phi_X$, or (b) the unknown distribution does not belong to $\Phi_X$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;hypothesis testing&lt;/strong&gt;, first such a model restriction is proposed, then the choice is made whether to accept the restriction or reject it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;estimation&lt;/strong&gt;, a restriction is to be chosen with certain level of certainty.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;Bayesian inference&lt;/strong&gt;, we already have a subjective restriction, and it is updated with the knowledge of the given sample.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is a &lt;strong&gt;sample&lt;/strong&gt;? A realization of a random vector.&lt;/p&gt;
&lt;h3&gt;Hypothesis testing&lt;/h3&gt;
&lt;p&gt;Quoting from the book:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hypothesis testing is a method of making statistical inferences by establishing an hypothesis, called null hypothesis, and using some data to decide whether to reject or not to reject the hypothesis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For parametric models, let $\Theta \subseteq \mathbb{R}^p$ be the parameter space. Let the true parameter of the unknown distribution be $\theta_0$. Let the chosen parameter restriction be $\Theta_R$. Then the &lt;strong&gt;null hypothesis&lt;/strong&gt; is defined as
$$H_0: \theta_0 \in \Theta_R$$&lt;/p&gt;
&lt;p&gt;And the &lt;strong&gt;alternative hyptothesis&lt;/strong&gt; is defined as
$$H_1: \theta_0 \in \Theta_R^c$$&lt;/p&gt;
&lt;h4&gt;Types of errors&lt;/h4&gt;
&lt;p&gt;Whether the null hypothesis is accepted or rejected, the decision may be wrong. Two types of errors exist:
1. Reject the null hypothesis when it is indeed true, called &lt;strong&gt;Type I error&lt;/strong&gt;.
2. Do not reject the null hypothesis when it is indeed false, called &lt;strong&gt;Type II error&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;~ (how can one not confuse these two terms? my trick is to think &lt;em&gt;conservatively&lt;/em&gt;: rejecting the true null hypothesis, which is often deemed as default is the main error, hence type I.)&lt;/p&gt;
&lt;h4&gt;Critical region&lt;/h4&gt;
&lt;p&gt;Is a subset of support where the null hypothesis is rejected when the sample observed happens to be from there. For hypothesis tests about the mean, the critical region could be a value above or below which has very low probability and the presence of the sample in it is a strong evidence agains the null hypothesis.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Q: How do we choose critical region? &lt;/p&gt;
&lt;p&gt;We can choose whatever, but the hypothesis test results can be wrong. I suppose, the "smaller" the critical region we choose, the more likely the test result will stick with null hypothesis. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the book:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The critical region is often defined implicitly through a &lt;strong&gt;test statistic&lt;/strong&gt; and a &lt;strong&gt;critical region&lt;/strong&gt; for the test statistic. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Test statistic&lt;/h4&gt;
&lt;p&gt;Is a function of sample (a quantity derived from the sample). Hence, it is a random variable. Its critical region is a subset of $\mathbb{R}$. Once one draws its critical region, it implies a certain critical region for the sample as well. The test will be based on test statistic:
$$
\text{test statistic is in critical region} \Rightarrow \text{sample is in critical region} \Rightarrow H_0 \text{ is rejected} 
$$$$
\text{test statistic not in critical region} \Rightarrow \text{sample not in critical region} \Rightarrow H_0 \text{ is not rejected}
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Q: Why use test statistic when defining a critical region is enough?&lt;/p&gt;
&lt;p&gt;I suppose, usually the test statistic is much easier to reason about and define a reasonable critical region. On the other hand, defining a critical region for the sample is impractical as you have multiple samples and each can be vectors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Power of test&lt;/h4&gt;
&lt;p&gt;Power of a test is the probability of rejecting the null hypothesis when, in fact, it is false. Power is a number between 0 and 1. Thus, the power of 1 indicates that the test is "powerful", i.e. it rejects the null hypothesis whenever it is false. In other terms, it indicates immunity from Type II error. 
Yes, it would be great to know the power of your test. So, how? &lt;/p&gt;
&lt;h4&gt;Power function&lt;/h4&gt;
&lt;p&gt;This is the generalization of the notion of power of test. While power of test is just a quantity, the power function is defined on every possible parameter $\theta$, in case of parametric tests. 
$$
\pi(\theta) = P(\text{sample falls in critical region} | \text{true distribution parameter is } \theta)
$$&lt;/p&gt;
&lt;p&gt;More on power function &lt;a href="https://www.statlect.com/glossary/power-function"&gt;here&lt;/a&gt; that has a simple example included:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose you are testing the null hypothesis that the true parameter is equal to zero:
$$H_0: \theta = 0$$
Suppose that the value of the power function at $\theta =1$ is
$$\pi(1) = 0.5$$
What does this mean? It means that if the true parameter is equal to 1, then there is a 50% probability that the test will reject the (false) null hypothesis that the parameter is equal to 0.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Q: Knowing the power function of your test is great. But how can you derive it?&lt;/p&gt;
&lt;h4&gt;Size of a test&lt;/h4&gt;
&lt;p&gt;Size of a test is a measure of incorrectly rejecting the null hypothesis. It is defined in terms of the power function:
$$
sup_{\theta \in \Theta_R} \pi(\theta)
$$
In other words, it is the upper bound probability of incorrectly rejecting the null hypothesis. Ideally, we want the size of test to be 0.&lt;/p&gt;</content><category term="misc"></category></entry><entry><title>Some statistical terms</title><link href="/some-statistical-terms.html" rel="alternate"></link><published>2021-05-01T00:00:00+03:00</published><updated>2021-05-01T00:00:00+03:00</updated><author><name>Sobir Bobiev</name></author><id>tag:None,2021-05-01:/some-statistical-terms.html</id><summary type="html">&lt;h3&gt;Joint probability distribution function&lt;/h3&gt;
&lt;p&gt;It characterizes the probability of a random vector in its domain.
So, for a random vector $X \in R^n$, it is a function 
$$f_X: R^n \to [0, \inf) .$$
Computing the integral over the support region should result in $1$.&lt;/p&gt;
&lt;p&gt;There is another term called …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Joint probability distribution function&lt;/h3&gt;
&lt;p&gt;It characterizes the probability of a random vector in its domain.
So, for a random vector $X \in R^n$, it is a function 
$$f_X: R^n \to [0, \inf) .$$
Computing the integral over the support region should result in $1$.&lt;/p&gt;
&lt;p&gt;There is another term called &lt;em&gt;joint distribution function&lt;/em&gt; which is a synonym to &lt;em&gt;joint cumulative distribution function&lt;/em&gt; (also &lt;em&gt;joint cdf&lt;/em&gt;). At a given point $x$ it gives the probability that all components of $X$ are smaller than the corresponding component $x$.&lt;/p&gt;
&lt;h3&gt;Marginal distribution function&lt;/h3&gt;
&lt;p&gt;For a random vector, it is the probability of one of its components. 
So, for a random vector $X \in R^n$ and its component $X_i$, it is a function 
$$f_{X_i}: R \to [0, \inf) .$$&lt;/p&gt;</content><category term="misc"></category></entry><entry><title>Contour detection</title><link href="/contour-detection.html" rel="alternate"></link><published>2021-04-30T00:00:00+03:00</published><updated>2021-04-30T00:00:00+03:00</updated><author><name>Sobir Bobiev</name></author><id>tag:None,2021-04-30:/contour-detection.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Dictionary definition: an outline representing or bounding the shape or form of something.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Separate bright object from dark background. Contours are polygons. Contours are the boundaries of
objects with the same intensity. &lt;/p&gt;
&lt;p&gt;From OpenCV:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Contours is a Python list of all the contours in the image. Each individual contour is …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Dictionary definition: an outline representing or bounding the shape or form of something.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Separate bright object from dark background. Contours are polygons. Contours are the boundaries of
objects with the same intensity. &lt;/p&gt;
&lt;p&gt;From OpenCV:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Contours is a Python list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In OpenCV, for efficiency purposes, the contours can be &lt;em&gt;appoximated&lt;/em&gt;. Without approximation, it
can return too many boundary points. With approximation, they can be reduced to much smaller number (e.g say 4 points for a rectangular object).&lt;/p&gt;
&lt;p&gt;Contours detected in an image form a &lt;em&gt;hierarchy&lt;/em&gt;. One can contain the other, and so on. Remember &lt;em&gt;contour maps&lt;/em&gt; from math. However black and white images produce only disjoint contours[I think so].&lt;/p&gt;</content><category term="misc"></category></entry><entry><title>test</title><link href="/test.html" rel="alternate"></link><published>2021-04-30T00:00:00+03:00</published><updated>2021-04-30T00:00:00+03:00</updated><author><name>Sobir Bobiev</name></author><id>tag:None,2021-04-30:/test.html</id><content type="html">&lt;p&gt;This is my first post on my new blog. &lt;/p&gt;
&lt;p&gt;Let's write some inline math $e^x$&lt;/p&gt;
&lt;p&gt;And more math (block)
$$\int_0^{x^t} f(t) dt$$&lt;/p&gt;</content><category term="misc"></category></entry><entry><title>WHAT is SQL</title><link href="/what-is-sql.html" rel="alternate"></link><published>2021-04-30T00:00:00+03:00</published><updated>2021-04-30T00:00:00+03:00</updated><author><name>Sobir Bobiev</name></author><id>tag:None,2021-04-30:/what-is-sql.html</id><summary type="html">&lt;h1&gt;The idea of SQL&lt;/h1&gt;
&lt;p&gt;Entities exist. Multiple entities of the same &lt;em&gt;kind&lt;/em&gt; exist. Example, many humans.&lt;/p&gt;
&lt;p&gt;All entities of the same kind have a set of attributes. Different entities differ 
because of attributes. Align those attributes in a &lt;em&gt;row&lt;/em&gt;. All entities 
are now represented by a row. then, A collection …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;The idea of SQL&lt;/h1&gt;
&lt;p&gt;Entities exist. Multiple entities of the same &lt;em&gt;kind&lt;/em&gt; exist. Example, many humans.&lt;/p&gt;
&lt;p&gt;All entities of the same kind have a set of attributes. Different entities differ 
because of attributes. Align those attributes in a &lt;em&gt;row&lt;/em&gt;. All entities 
are now represented by a row. then, A collection of entities is a &lt;em&gt;table&lt;/em&gt;.&lt;/p&gt;</content><category term="misc"></category></entry></feed>